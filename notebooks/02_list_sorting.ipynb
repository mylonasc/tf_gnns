{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a5o-b8uYkdun",
    "outputId": "d113b0a3-1ea3-49aa-f6ff-c094f933704c"
   },
   "outputs": [],
   "source": [
    "!pip install tf_gnns\n",
    "!pip install --quiet 'networkx<3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "cellView": "form",
    "id": "kL37wyGwkmvX"
   },
   "outputs": [],
   "source": [
    "#@title ##### `tf_gnns` license\n",
    "# Copyright 2021, Mylonas Charilaos. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#    http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "# ##### `DeepMind GraphNets` license\n",
    "# Copyright 2018 The GraphNets Authors. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#    http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ============================================================================\n",
    "\n",
    "###############################################################################\n",
    "# Changes to the original file:\n",
    "# ------------------------------\n",
    "# This notebook is a modified version of the `deepmind/graph_nets` example code \n",
    "# which demonstrates the use of graph nets to list sorting. The parts of code \n",
    "# from the original file are the sampling of GraphTuples which contain the input \n",
    "# and output graphs. The parts that are different are the parts needed to use \n",
    "# the tf_gnns library instead of the graph nets. More speciffically, a function \n",
    "# that casts graphtuples to tensor_dicts is created, and an EncodeProcessDecode \n",
    "# network almost identical to the one used in the paper is created.\n",
    "# Visualization is also different from the original file, yet very similar.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RwPNgY5llFZk"
   },
   "source": [
    "# List sorting with GNNs\n",
    "A large part of the code for this example is taken from the [deepmind/graph_nets](https://github.com/deepmind/graph_nets/blob/master/graph_nets/) library (the Apache licence of `graph_nets` is also attached). In this notebook, the exact code that generates the data used in the DeepMind `graph_nets` example is used, but the GN used for processing is usgin the `tf_gnns` functionality (no tf 1.x or Sonnet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m-xnfADflH5n",
    "outputId": "6d2e7407-cfb5-4f98-d29b-16300b60c91d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing Graph Nets library and dependencies:\n",
      "Output message from command:\n",
      "\n",
      "Requirement already satisfied: graph_nets>=1.1 in /mnt/6c99ccdb-4d5e-443d-b361-8b5137623ef6/tf_gnns_devops/.venv/lib/python3.10/site-packages (1.1.0)\n",
      "Requirement already satisfied: dm-sonnet>=2.0.0b0 in /mnt/6c99ccdb-4d5e-443d-b361-8b5137623ef6/tf_gnns_devops/.venv/lib/python3.10/site-packages (2.0.2)\n",
      "Requirement already satisfied: absl-py in /mnt/6c99ccdb-4d5e-443d-b361-8b5137623ef6/tf_gnns_devops/.venv/lib/python3.10/site-packages (from graph_nets>=1.1) (2.2.2)\n",
      "Requirement already satisfied: dm-tree in /mnt/6c99ccdb-4d5e-443d-b361-8b5137623ef6/tf_gnns_devops/.venv/lib/python3.10/site-packages (from graph_nets>=1.1) (0.1.9)\n",
      "Requirement already satisfied: future in /mnt/6c99ccdb-4d5e-443d-b361-8b5137623ef6/tf_gnns_devops/.venv/lib/python3.10/site-packages (from graph_nets>=1.1) (1.0.0)\n",
      "Requirement already satisfied: networkx in /mnt/6c99ccdb-4d5e-443d-b361-8b5137623ef6/tf_gnns_devops/.venv/lib/python3.10/site-packages (from graph_nets>=1.1) (2.8.8)\n",
      "Requirement already satisfied: numpy in /mnt/6c99ccdb-4d5e-443d-b361-8b5137623ef6/tf_gnns_devops/.venv/lib/python3.10/site-packages (from graph_nets>=1.1) (2.1.3)\n",
      "Requirement already satisfied: setuptools in /mnt/6c99ccdb-4d5e-443d-b361-8b5137623ef6/tf_gnns_devops/.venv/lib/python3.10/site-packages (from graph_nets>=1.1) (59.6.0)\n",
      "Requirement already satisfied: six in /mnt/6c99ccdb-4d5e-443d-b361-8b5137623ef6/tf_gnns_devops/.venv/lib/python3.10/site-packages (from graph_nets>=1.1) (1.17.0)\n",
      "Requirement already satisfied: tabulate>=0.7.5 in /mnt/6c99ccdb-4d5e-443d-b361-8b5137623ef6/tf_gnns_devops/.venv/lib/python3.10/site-packages (from dm-sonnet>=2.0.0b0) (0.9.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /mnt/6c99ccdb-4d5e-443d-b361-8b5137623ef6/tf_gnns_devops/.venv/lib/python3.10/site-packages (from dm-sonnet>=2.0.0b0) (1.17.2)\n",
      "Requirement already satisfied: attrs>=18.2.0 in /mnt/6c99ccdb-4d5e-443d-b361-8b5137623ef6/tf_gnns_devops/.venv/lib/python3.10/site-packages (from dm-tree->graph_nets>=1.1) (25.3.0)\n"
     ]
    }
   ],
   "source": [
    "install_graph_nets_library = \"Yes\"  #@param [\"Yes\", \"No\"]\n",
    "\n",
    "if install_graph_nets_library.lower() == \"yes\":\n",
    "  print(\"Installing Graph Nets library and dependencies:\")\n",
    "  print(\"Output message from command:\\n\")\n",
    "  !pip install \"graph_nets>=1.1\" \"dm-sonnet>=2.0.0b0\"\n",
    "else:\n",
    "  print(\"Skipping installation of Graph Nets library\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CZ_q7g_OlJDa"
   },
   "outputs": [],
   "source": [
    "#@title Imports\n",
    "\n",
    "## Needed for manipulating the samplers from the graph_nets library (and making \n",
    "#  a 1-to-1 comparisson of the two frameworks)\n",
    "\n",
    "from graph_nets import utils_np\n",
    "from graph_nets import utils_tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tf_gnns.models.graphnet import GraphNetMLP, GNCellMLP, GraphIndep\n",
    "import tensorflow as tf\n",
    "\n",
    "SEED = 1\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "VtKMsM0ElM4D"
   },
   "outputs": [],
   "source": [
    "#@title Helper functions  { form-width: \"30%\" }\n",
    "\n",
    "# pylint: disable=redefined-outer-name\n",
    "\n",
    "def create_graph_dicts_tf(num_examples, num_elements_min_max):\n",
    "  \"\"\"Generate graphs for training.\n",
    "\n",
    "  Args:\n",
    "    num_examples: total number of graphs to generate\n",
    "    num_elements_min_max: a 2-tuple with the minimum and maximum number of\n",
    "      values allowable in a graph. The number of values for a graph is\n",
    "      uniformly sampled withing this range. The upper bound is exclusive, and\n",
    "      should be at least 2 more than the lower bound.\n",
    "\n",
    "  Returns:\n",
    "    inputs: contains the generated random numbers as node values.\n",
    "    sort_indices: contains the sorting indices as nodes. Concretely\n",
    "      inputs.nodes[sort_indices.nodes] will be a sorted array.\n",
    "    ranks: the rank of each value in inputs normalized to the range [0, 1].\n",
    "  \"\"\"\n",
    "  num_elements = tf.random.uniform(\n",
    "      [num_examples],\n",
    "      minval=num_elements_min_max[0],\n",
    "      maxval=num_elements_min_max[1],\n",
    "      dtype=tf.int32)\n",
    "  inputs_graphs = []\n",
    "  sort_indices_graphs = []\n",
    "  ranks_graphs = []\n",
    "  for i in range(num_examples):\n",
    "    values = tf.random.uniform(shape=[num_elements[i]])\n",
    "    sort_indices = tf.cast(\n",
    "        tf.argsort(values, axis=-1), tf.float32)\n",
    "    ranks = tf.cast(\n",
    "        tf.argsort(sort_indices, axis=-1), tf.float32) / (\n",
    "            tf.cast(num_elements[i], tf.float32) - 1.0)\n",
    "    inputs_graphs.append({\"nodes\": values[:, None]})\n",
    "    sort_indices_graphs.append({\"nodes\": sort_indices[:, None]})\n",
    "    ranks_graphs.append({\"nodes\": ranks[:, None]})\n",
    "  return inputs_graphs, sort_indices_graphs, ranks_graphs\n",
    "\n",
    "\n",
    "def create_linked_list_target(batch_size, input_graphs):\n",
    "  \"\"\"Creates linked list targets.\n",
    "\n",
    "  Returns a graph with the same number of nodes as `input_graph`. Each node\n",
    "  contains a 2d vector with targets for a 1-class classification where only one\n",
    "  node is `True`, the smallest value in the array. The vector contains two\n",
    "  values: [prob_true, prob_false].\n",
    "  It also contains edges connecting all nodes. These are again 2d vectors with\n",
    "  softmax targets [prob_true, prob_false]. An edge is True\n",
    "  if n+1 is the element immediately after n in the sorted list.\n",
    "\n",
    "  Args:\n",
    "    batch_size: batch size for the `input_graphs`.\n",
    "    input_graphs: a `graphs.GraphsTuple` which contains a batch of inputs.\n",
    "\n",
    "  Returns:\n",
    "    A `graphs.GraphsTuple` with the targets, which encode the linked list.\n",
    "  \"\"\"\n",
    "  target_graphs = []\n",
    "  for i in range(batch_size):\n",
    "    input_graph = utils_tf.get_graph(input_graphs, i)\n",
    "    num_elements = tf.shape(input_graph.nodes)[0]\n",
    "    si = tf.cast(tf.squeeze(input_graph.nodes), tf.int32)\n",
    "    nodes = tf.reshape(tf.one_hot(si[:1], num_elements), (-1, 1))\n",
    "    x = tf.stack((si[:-1], si[1:]))[None]\n",
    "    y = tf.stack(\n",
    "        (input_graph.senders, input_graph.receivers), axis=1)[:, :, None]\n",
    "    edges = tf.reshape(\n",
    "        tf.cast(\n",
    "            tf.reduce_any(tf.reduce_all(tf.equal(x, y), axis=1), axis=1),\n",
    "            tf.float32), (-1, 1))\n",
    "    target_graphs.append(input_graph._replace(nodes=nodes, edges=edges))\n",
    "  return utils_tf.concat(target_graphs, axis=0)\n",
    "\n",
    "\n",
    "def compute_accuracy(target, output):\n",
    "  \"\"\"Calculate model accuracy.\n",
    "\n",
    "  Returns the number of correctly predicted links and the number\n",
    "  of completely solved list sorts (100% correct predictions).\n",
    "\n",
    "  Args:\n",
    "    target: A `graphs.GraphsTuple` that contains the target graph.\n",
    "    output: A `graphs.GraphsTuple` that contains the output graph.\n",
    "\n",
    "  Returns:\n",
    "    correct: A `float` fraction of correctly labeled nodes/edges.\n",
    "    solved: A `float` fraction of graphs that are completely correctly labeled.\n",
    "  \"\"\"\n",
    "  tdds = utils_np.graphs_tuple_to_data_dicts(target)\n",
    "  odds = utils_np.graphs_tuple_to_data_dicts(output)\n",
    "  cs = []\n",
    "  ss = []\n",
    "  for td, od in zip(tdds, odds):\n",
    "    num_elements = td[\"nodes\"].shape[0]\n",
    "    xn = np.argmax(td[\"nodes\"], axis=-1)\n",
    "    yn = np.argmax(od[\"nodes\"], axis=-1)\n",
    "\n",
    "    xe = np.reshape(\n",
    "        np.argmax(\n",
    "            np.reshape(td[\"edges\"], (num_elements, num_elements, 2)), axis=-1),\n",
    "        (-1,))\n",
    "    ye = np.reshape(\n",
    "        np.argmax(\n",
    "            np.reshape(od[\"edges\"], (num_elements, num_elements, 2)), axis=-1),\n",
    "        (-1,))\n",
    "    c = np.concatenate((xn == yn, xe == ye), axis=0)\n",
    "    s = np.all(c)\n",
    "    cs.append(c)\n",
    "    ss.append(s)\n",
    "  correct = np.mean(np.concatenate(cs, axis=0))\n",
    "  solved = np.mean(np.stack(ss))\n",
    "  return correct, solved\n",
    "\n",
    "\n",
    "def create_data(batch_size, num_elements_min_max):\n",
    "  \"\"\"Returns graphs containing the inputs and targets for classification.\n",
    "\n",
    "  Refer to create_data_dicts_tf and create_linked_list_target for more details.\n",
    "\n",
    "  Args:\n",
    "    batch_size: batch size for the `input_graphs`.\n",
    "    num_elements_min_max: a 2-`tuple` of `int`s which define the [lower, upper)\n",
    "      range of the number of elements per list.\n",
    "\n",
    "  Returns:\n",
    "    inputs: a `graphs.GraphsTuple` which contains the input list as a graph.\n",
    "    targets: a `graphs.GraphsTuple` which contains the target as a graph.\n",
    "    sort_indices: a `graphs.GraphsTuple` which contains the sort indices of\n",
    "      the list elements a graph.\n",
    "    ranks: a `graphs.GraphsTuple` which contains the ranks of the list\n",
    "      elements as a graph.\n",
    "  \"\"\"\n",
    "  inputs, sort_indices, ranks = create_graph_dicts_tf(\n",
    "      batch_size, num_elements_min_max)\n",
    "  inputs = utils_tf.data_dicts_to_graphs_tuple(inputs)\n",
    "  sort_indices = utils_tf.data_dicts_to_graphs_tuple(sort_indices)\n",
    "  ranks = utils_tf.data_dicts_to_graphs_tuple(ranks)\n",
    "\n",
    "  inputs = utils_tf.fully_connect_graph_dynamic(inputs)\n",
    "  sort_indices = utils_tf.fully_connect_graph_dynamic(sort_indices)\n",
    "  ranks = utils_tf.fully_connect_graph_dynamic(ranks)\n",
    "\n",
    "  targets = create_linked_list_target(batch_size, sort_indices)\n",
    "  nodes = tf.concat((targets.nodes, 1.0 - targets.nodes), axis=1)\n",
    "  edges = tf.concat((targets.edges, 1.0 - targets.edges), axis=1)\n",
    "  targets = targets._replace(nodes=nodes, edges=edges)\n",
    "\n",
    "  return inputs, targets, sort_indices, ranks\n",
    "\n",
    "\n",
    "def create_loss(target, outputs):\n",
    "  \"\"\"Returns graphs containing the inputs and targets for classification.\n",
    "\n",
    "  Refer to create_data_dicts_tf and create_linked_list_target for more details.\n",
    "\n",
    "  Args:\n",
    "    target: a `graphs.GraphsTuple` which contains the target as a graph.\n",
    "    outputs: a `list` of `graphs.GraphsTuple`s which contains the model\n",
    "      outputs for each processing step as graphs.\n",
    "\n",
    "  Returns:\n",
    "    A `list` of ops which are the loss for each processing step.\n",
    "  \"\"\"\n",
    "  # if not isinstance(outputs, collections.Sequence):\n",
    "  #   outputs = [outputs]\n",
    "  losss = [\n",
    "      tf.compat.v1.losses.softmax_cross_entropy(target.nodes, output.nodes) +\n",
    "      tf.compat.v1.losses.softmax_cross_entropy(target.edges, output.edges)\n",
    "      for output in outputs\n",
    "  ]\n",
    "  return tf.stack(losss)\n",
    "\n",
    "\n",
    "\n",
    "def plot_linked_list(ax, graph, sort_indices):\n",
    "  \"\"\"Plot a networkx graph containing weights for the linked list probability.\"\"\"\n",
    "  nd = len(graph.nodes())\n",
    "  probs = np.zeros((nd, nd))\n",
    "  for edge in graph.edges(data=True):\n",
    "    probs[edge[0], edge[1]] = edge[2][\"features\"][0]\n",
    "  ax.matshow(probs[sort_indices][:, sort_indices], cmap=\"viridis\")\n",
    "  ax.grid(False)\n",
    "\n",
    "\n",
    "# pylint: enable=redefined-outer-name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 567
    },
    "id": "qdT-cxjolP-M",
    "outputId": "acec6934-b092-4560-8a4d-37672ee57996"
   },
   "outputs": [],
   "source": [
    "#@title Visualize the sort task  { form-width: \"30%\" }\n",
    "num_elements_min_max = (5, 10)\n",
    "\n",
    "inputs, targets, sort_indices, ranks = create_data(\n",
    "    1, num_elements_min_max)\n",
    "\n",
    "inputs_nodes = inputs.nodes.numpy()\n",
    "targets = utils_tf.nest_to_numpy(targets)\n",
    "sort_indices_nodes = sort_indices.nodes.numpy()\n",
    "ranks_nodes = ranks.nodes.numpy()\n",
    "\n",
    "sort_indices = np.squeeze(sort_indices_nodes).astype(int)\n",
    "\n",
    "# Plot sort linked lists.\n",
    "# The matrix plots show each element from the sorted list (rows), and which\n",
    "# element they link to as next largest (columns). Ground truth is a diagonal\n",
    "# offset toward the upper-right by one.\n",
    "fig = plt.figure(1, figsize=(4, 4))\n",
    "fig.clf()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "plot_linked_list(ax,\n",
    "                 utils_np.graphs_tuple_to_networkxs(targets)[0], sort_indices)\n",
    "ax.set_title(\"Element-to-element links for sorted elements\")\n",
    "ax.set_axis_off()\n",
    "\n",
    "fig = plt.figure(2, figsize=(10, 2))\n",
    "fig.clf()\n",
    "ax1 = fig.add_subplot(1, 3, 1)\n",
    "ax2 = fig.add_subplot(1, 3, 2)\n",
    "ax3 = fig.add_subplot(1, 3, 3)\n",
    "\n",
    "i = 0\n",
    "num_elements = ranks_nodes.shape[0]\n",
    "inputs = np.squeeze(inputs_nodes)\n",
    "ranks = np.squeeze(ranks_nodes * (num_elements - 1.0)).astype(int)\n",
    "x = np.arange(inputs.shape[0])\n",
    "\n",
    "ax1.set_title(\"Inputs\")\n",
    "ax1.barh(x, inputs, color=\"b\")\n",
    "ax1.set_xlim(-0.01, 1.01)\n",
    "\n",
    "ax2.set_title(\"Sorted\")\n",
    "ax2.barh(x, inputs[sort_indices], color=\"k\")\n",
    "ax2.set_xlim(-0.01, 1.01)\n",
    "\n",
    "ax3.set_title(\"Ranks\")\n",
    "ax3.barh(x, ranks, color=\"r\")\n",
    "_ = ax3.set_xlim(0, len(ranks) + 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uZ0o-4iQlTvb"
   },
   "outputs": [],
   "source": [
    "from tf_gnns.lib.gt_ops import _concat_tensordicts\n",
    "from tf_gnns.lib.gt_ops import _add_gt\n",
    "class EncodeProcessDecode(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    The same model as in graph_nets library (16 layer wide, 2-layer ReLU MLPs \n",
    "    with activation at the last layer and layer normalization as MLPs):\n",
    "\n",
    "    Graphic from `graph_nets` library: \n",
    "\n",
    "                        Hidden(t)   Hidden(t+1)\n",
    "                         |            ^\n",
    "            *---------*  |  *------*  |  *---------*\n",
    "            |         |  |  |      |  |  |         |\n",
    "  Input --->| Encoder |  *->| Core |--*->| Decoder |---> Output(t)\n",
    "            |         |---->|      |     |         |\n",
    "            *---------*     *------*     *---------*\n",
    "\n",
    "    In deepmind/graph_nets there is an additional linear GraphIndependent layer \n",
    "    to the decoder with dense MLPs than the ones shown here. \n",
    "    \"\"\"\n",
    "    def __init__(self, num_processing_steps = 10, num_units=16):\n",
    "\n",
    "        super(EncodeProcessDecode,self).__init__()\n",
    "        self.gn_enc = GraphIndep(num_units, gn_mlp_units = [num_units,num_units],\n",
    "                                  layernorm_last_layer = True, \n",
    "                                  activate_last_layer = True)\n",
    "        \n",
    "        self.gn_core   = GNCellMLP(num_units, core_size=num_units,\n",
    "                                  aggregation_function = 'sum',\n",
    "                                layernorm_last_layer = True,\n",
    "                                activate_last_layer = True)\n",
    "        \n",
    "        # The following two layers are used to make an implementation closer \n",
    "        # to what the deepmind/graph_nets library implements.\n",
    "        # In tf_gnns one may alteratively use a single decoder that outputs \n",
    "        # 2-dim outputs using ... \"gn_mlp_units = [num_units, num_units, 2]\"\n",
    "        self.gn_dec = GraphIndep(num_units,gn_mlp_units = [num_units,num_units],\n",
    "                                 layernorm_last_layer = True, \n",
    "                                  activate_last_layer = True)\n",
    "        \n",
    "        self.gn_output_transform = GraphIndep(2, []) # nodes and edges are both with output\n",
    "                                        # size 2.\n",
    "\n",
    "        self.num_processing_steps = num_processing_steps\n",
    "\n",
    "    def call(self, graph_in):\n",
    "        \n",
    "        g_ = self.gn_enc(graph_in)\n",
    "        g_0_ = g_.copy()\n",
    "        \n",
    "        outputs = [];\n",
    "        for ii in range(self.num_processing_steps):\n",
    "            g_ = self.gn_core(_concat_tensordicts(g_, g_0_))\n",
    "            g_to_dec = self.gn_dec(g_)\n",
    "            outputs.append(self.gn_output_transform(g_to_dec))\n",
    "        \n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ET7VvKGKlT5y"
   },
   "outputs": [],
   "source": [
    "#@title Set up model training and evaluation  { form-width: \"30%\" }\n",
    "\n",
    "\n",
    "# Model parameters.\n",
    "# Number of processing (message-passing) steps.\n",
    "num_processing_steps = 10\n",
    "# Data / training parameters.\n",
    "num_training_iterations = 6000\n",
    "batch_size_tr = 32\n",
    "batch_size_ge = 100\n",
    "# Number of elements in each list is sampled uniformly from this range.\n",
    "num_elements_min_max_tr = (8, 17)\n",
    "num_elements_min_max_ge = (16, 33)\n",
    "\n",
    "# Data.\n",
    "if 'get_data' not in  locals().keys():\n",
    "    @tf.function\n",
    "    def get_data():\n",
    "        inputs_tr, targets_tr, sort_indices_tr, _ = create_data(\n",
    "            batch_size_tr, num_elements_min_max_tr)\n",
    "        inputs_tr = utils_tf.set_zero_edge_features(inputs_tr, 1)\n",
    "        inputs_tr = utils_tf.set_zero_global_features(inputs_tr, 1)\n",
    "        # Test/generalization.\n",
    "        inputs_ge, targets_ge, sort_indices_ge, _ = create_data(\n",
    "            batch_size_ge, num_elements_min_max_ge)\n",
    "        inputs_ge = utils_tf.set_zero_edge_features(inputs_ge, 1)\n",
    "        inputs_ge = utils_tf.set_zero_global_features(inputs_ge, 1)\n",
    "\n",
    "        targets_tr = utils_tf.set_zero_global_features(targets_tr, 1)\n",
    "        targets_ge = utils_tf.set_zero_global_features(targets_ge, 1)\n",
    "\n",
    "        return inputs_tr, targets_tr, sort_indices_tr, inputs_ge, targets_ge, sort_indices_ge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "prnMotaBxJXF"
   },
   "outputs": [],
   "source": [
    "inputs_tr, targets_tr, sort_indices_tr, inputs_ge, targets_ge, sort_indices_ge = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lY9bUGUVxYT4"
   },
   "outputs": [],
   "source": [
    "def to_tensor_dict(v):\n",
    "    vout = {'senders' : v.senders, \n",
    "    'receivers' : v.receivers,\n",
    "    'edges' : v.edges,\n",
    "    'nodes' : v.nodes,\n",
    "    'n_nodes' : v.n_node, \n",
    "    'n_edges' : v.n_edge, \n",
    "    'global_attr':v.globals,\n",
    "    'global_reps_for_nodes' : tf.repeat(tf.range(v.n_node.shape[0]),v.n_node),\n",
    "    'global_reps_for_edges' : tf.repeat(tf.range(v.n_edge.shape[0]), v.n_edge),\n",
    "    'n_graphs' : tf.shape(v.n_node)[0]\n",
    "    }\n",
    "    return vout\n",
    "\n",
    "def get_data_td():\n",
    "    in_tr, out_tr, sort_tr, in_ge, out_ge, inds_ge = get_data()\n",
    "    return to_tensor_dict(in_tr), to_tensor_dict(out_tr), sort_tr, to_tensor_dict(in_ge), to_tensor_dict(out_ge), inds_ge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xerm1l8EHwcd",
    "outputId": "de490377-e2f5-4e05-ee6d-5cabb4125a21"
   },
   "outputs": [],
   "source": [
    "model = EncodeProcessDecode(num_processing_steps=num_processing_steps)\n",
    "\n",
    "in_tr, out_tr, _,_,_,_ = get_data_td()\n",
    "out = model(in_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k2uy776FIe_m"
   },
   "outputs": [],
   "source": [
    "losses = []\n",
    "accuracy_train, accuracy_test = [[],[]]\n",
    "opt = tf.keras.optimizers.Adam(learning_rate = 1e-3, epsilon = 1e-8, clipnorm=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oDhFYIJxEk5S"
   },
   "outputs": [],
   "source": [
    "def accuracy_correct_sort(nbatches = 1):\n",
    "    \"\"\"\n",
    "    Computes for the train and test set how many complete sequences are correctly \n",
    "    sorted.\n",
    "    \"\"\"\n",
    "    pct_correct_graphs_g = pct_correct_graphs_t = 0.\n",
    "    for tt in range(nbatches):\n",
    "        in_t, out_t, s_t, in_g, out_g, s_g = get_data_td()\n",
    "        out_g_hat = model(in_g)[-1]\n",
    "        out_t_hat = model(in_t)[-1]\n",
    "        pct_correct_graphs_g += tf.reduce_sum(tf.math.unsorted_segment_prod(tf.cast(tf.argmax(out_g_hat['edges'],1) == tf.argmax(out_g['edges'],1), tf.int32),out_g['global_reps_for_edges'], out_g['n_graphs']))/out_g['n_graphs']\n",
    "        pct_correct_graphs_t += tf.reduce_sum(tf.math.unsorted_segment_prod(tf.cast(tf.argmax(out_t_hat['edges'],1) == tf.argmax(out_t['edges'],1), tf.int32),out_t['global_reps_for_edges'], out_t['n_graphs']))/out_t['n_graphs']\n",
    "    return pct_correct_graphs_g/nbatches, pct_correct_graphs_t/nbatches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RoRZOEKox1KW"
   },
   "outputs": [],
   "source": [
    "import IPython\n",
    "@tf.function\n",
    "def eval_loss(input, target):\n",
    "    out = model(input)\n",
    "    edge_loss = 0.\n",
    "    node_loss = 0.\n",
    "    for o in out:\n",
    "        edge_loss += tf.nn.softmax_cross_entropy_with_logits(target['edges'],o['edges'])  \n",
    "        node_loss += tf.nn.softmax_cross_entropy_with_logits(target['nodes'],o['nodes'])\n",
    "    return tf.reduce_mean(node_loss) + tf.reduce_mean(edge_loss)\n",
    "\n",
    "@tf.function\n",
    "def train_step():\n",
    "    in_t, out_t, out_inds_t,in_v,out_v, out_inds_g = get_data_td()\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = eval_loss(in_t, out_t)\n",
    "        grad = tape.gradient(loss, model.weights)\n",
    "        opt.apply_gradients(zip(grad, model.weights))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-ivXS5UFI6ng"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as pplot\n",
    "from tqdm import tqdm  \n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bYoCBg7UTUQi"
   },
   "outputs": [],
   "source": [
    "in_,out_, s_ , _,_,_ = get_data_td()\n",
    "out_h_ = model(in_)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uzYnmdSLun4f"
   },
   "outputs": [],
   "source": [
    "step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 695
    },
    "collapsed": true,
    "id": "y6S48_JkFoyK",
    "outputId": "90a30f33-db2d-4300-fcb1-090ad41c5abd"
   },
   "outputs": [],
   "source": [
    "\n",
    "if 'start_iteration' not in locals():\n",
    "    start_iteration = 0\n",
    "else:\n",
    "    start_iteration = step\n",
    "\n",
    "for step in tqdm(range(start_iteration,num_training_iterations)):\n",
    "    loss = train_step()\n",
    "    losses.append(loss)\n",
    "    if step % 100 == 0:\n",
    "        clear_output()\n",
    "        pplot.figure(figsize = (10,5), dpi = 150)\n",
    "        test_acc, train_acc = accuracy_correct_sort()\n",
    "        accuracy_test.append( test_acc)\n",
    "        accuracy_train.append(train_acc)\n",
    "        pplot.subplot(1,2,1)\n",
    "        pplot.plot(losses)\n",
    "        pplot.title(\"Loss (cross entropy)\")\n",
    "        pplot.subplot(1,2,2)\n",
    "        pplot.plot(accuracy_train, label = \"train acc\")\n",
    "        pplot.grid()\n",
    "        pplot.plot(accuracy_test, label = \"test acc\")\n",
    "        pplot.grid()\n",
    "        \n",
    "        pplot.legend()\n",
    "        pplot.title(\"correctly sorted ratio\")\n",
    "        pplot.pause(0.1)\n",
    "        pplot.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note\n",
    "In earlier versions of this notebook there was a comparisson with DeepMind GraphNets showing comparable performance. \n",
    "\n",
    "DeepMind does not seem to maintain the repo anymore, so there is no comparisson."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "tf_gnns_sorting_example.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
